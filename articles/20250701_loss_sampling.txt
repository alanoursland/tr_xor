Loss-Based Sampling Initialization for Neural Networks: Theory and Experimental Validation

Introduction
This work presents a simple yet effective initialization strategy for neural networks: sample multiple random initializations, evaluate their initial loss, and select from those with the lowest values. The approach emerges from our prior finding that convergence time is linearly proportional to parameter distance from solution in well-behaved optimization landscapes. We validate this method on ReLU networks, demonstrating improvements ranging from complete problem solution to substantial gains even in pathological cases.

Theoretical Foundation
Our previous work on a minimal model (y = |Wx + b|) revealed a striking linear relationship between parameter distance and convergence time, with R² = 0.9666. This finding suggests that:

1. Convergence time = α × distance_to_solution + β

2. Lower initial loss correlates with smaller distance to solution

3. Selecting low-loss initializations should reduce training time

The key insight is that in optimization landscapes where convergence is guaranteed and no deceptive local minima exist, initial loss serves as a reliable proxy for distance to solution. Therefore, sampling for low initial loss directly optimizes for faster convergence.

Method
The proposed algorithm is straightforward:

1. Sample N random initializations (typically 100-1000)

2. Evaluate initial loss for each using a forward pass

3. Select from the bottom percentile of the loss distribution

4. Train from the selected initialization

For our experiments, we use an adaptive variant: sample random initializations until finding one with loss below a predetermined threshold (e.g., better than the best of 100 random samples).

The computational cost is minimal - N forward passes - compared to the potential savings of hundreds of training epochs. The method requires no architectural changes and works with any optimizer or loss function.

Experimental Setup
We tested the approach on two ReLU network architectures solving the centered XOR problem:

Model 1 - Single-Layer ReLU:
- Architecture: y = ReLU(W₀x + b₀) + ReLU(W₁x + b₁)

- Parameters: 6 total (two 2D weight vectors plus biases)

- Optimizer: Adam

- Loss: MSE

Model 2 - Two-Layer ReLU:
- Architecture: Linear(2,2) → ReLU → Linear(2,2)

- Parameters: 10 total

- Output: One-hot encoding

- Optimizer: SGD

- Loss: MSE

Both models were trained until loss < 1e-7 or 600-800 epochs maximum. Success is defined as achieving 100% classification accuracy on the four XOR points.

Results: Single-Layer ReLU Network
Initial Loss Distribution (100 samples):
- Mean: 0.581

- Median: 0.468

- 25th percentile: 0.325

- 0th percentile (minimum): 0.072

Performance comparison across 50 runs each:
Control (random initialization):

- Success rate: 58% (29/50)

- Median epochs to convergence: 123

- Range: 33-275 epochs

Below 50th percentile (< 0.468):

- Success rate: 70% (35/50)

- Median epochs: 101

- Improvement: +21% success rate

Below 25th percentile (< 0.325):

- Success rate: 86% (43/50)

- Median epochs: 105

- Improvement: +48% success rate

Below 0th percentile (< 0.072):

- Success rate: 100% (50/50)

- Median epochs: 49

- Range: 25-99 epochs

- Improvement: +72% success rate, 2.5× speedup

Inverse validation (> 50th percentile):

- Success rate: 28% (14/50)

- Confirms that high initial loss predicts poor performance

The results show a clear monotonic relationship: lower initial loss leads to higher success rates and faster convergence. The complete elimination of training failures with best-percentile initialization is particularly striking.

Results: Two-Layer ReLU Network
Initial Loss Distribution (100 samples):
- Mean: 1.36

- Median: 0.908

- 25th percentile: 0.644

- 0th percentile: 0.326

This network starts from a worse position - no random initializations achieved 100% accuracy before training, and the initial loss is roughly 2× higher than the single-layer model.

Performance comparison:
Control (random initialization):

- Success rate: 20% (10/50)

- 10th percentile epochs: 410

- Most runs hit 600 epoch limit at 75% accuracy

Below 0th percentile (< 0.326):

- Success rate: 38% (19/50)

- 10th percentile epochs: 379

- Improvement: +90% relative increase in success rate

While the improvement is substantial (nearly doubling the success rate), the two-layer network remains challenging. The majority of runs still converge to a local minimum at loss ≈ 0.125, corresponding to outputting [0.5, 0.5] for all inputs.

Analysis and Interpretation
Connection to Absolute Value Decomposition
The single-layer ReLU model has an interesting property: |x| = ReLU(x) + ReLU(-x). Our architecture can exactly represent the absolute value function through mirror-symmetric weights. Analysis shows:

- Random initialization: 58% of successful runs find mirror pairs

- Smart initialization: 100% of successful runs find mirror pairs

This suggests good initializations preserve the possibility of discovering the optimal functional decomposition.

Why the Method Works
The effectiveness of sampling initialization confirms our theoretical predictions:

1. Distance determines difficulty: The linear relationship between distance and convergence time means starting closer directly reduces training time.

2. Avoiding pathologies: In ReLU networks, poor initializations can lead to dead neurons. Good initializations maintain active gradients.

3. Escaping plateaus: Lower initial loss helps avoid regions where gradients are uninformative.

Limitations and Failure Modes

The two-layer results highlight important limitations:

1. Architecture matters: Multiple layers create compound difficulties that initialization alone cannot solve.

2. Optimizer choice: SGD lacks the adaptive properties needed to escape certain local minima, regardless of initialization.

3. Loss landscape complexity: When multiple distinct failure modes exist, sampling initialization helps but cannot guarantee success.

The method is most effective when:

- The primary challenge is distance rather than landscape complexity

- The architecture has a clear path to the solution

- An appropriate optimizer is used

Comparison with Related Work

Our results align with and extend previous empirical findings. Fort & Scherlis (2019) reported 20-35% speedup using first-batch loss selection. We achieve greater improvements through:

- More aggressive filtering (0th percentile vs. simple selection)

- Theoretical grounding in the distance-convergence relationship

- Adaptive sampling to guarantee quality thresholds

The key contribution is providing theoretical justification for why these methods work, based on the linear relationship between parameter distance and convergence time.

Practical Considerations
Implementation is straightforward:

1. Sampling cost: For networks where a forward pass is fast relative to training, sampling 100-1000 initializations is negligible.

2. Threshold selection: Using percentiles of the empirical distribution adapts to problem difficulty automatically.

3. Scaling: For large networks, storing full parameter sets may be prohibitive. Instead, store random seeds and regenerate.

Expected sampling requirements:

- 50th percentile: ~2 samples

- 25th percentile: ~4 samples

- 0th percentile: ~100 samples

The cost-benefit clearly favors sampling when training takes hundreds of epochs.

Future Directions
Several extensions merit investigation:

1. Theoretical analysis: Formal proofs of convergence improvement under specific assumptions.

2. Scale studies: How does effectiveness change with network size and depth?

3. Combination methods: Integrating sampling with other initialization schemes.

4. Adaptive sampling: Dynamically adjusting thresholds based on observed convergence.

5. Architecture co-design: Building networks specifically amenable to sampling initialization.

Conclusions
We have demonstrated that sampling for low initial loss provides a simple and effective initialization strategy for neural networks. The method:

- Achieved 100% success rate on a problem that typically fails 42% of the time

- Reduced median training time by 60% for successful runs

- Nearly doubled success rates even in pathological cases

- Requires minimal computational overhead

These results validate our theoretical prediction that convergence time is proportional to initial parameter distance. Many perceived "hard to train" networks may simply start too far from good solutions. By sampling for favorable starting positions, we can make training more reliable and predictable.

While not a panacea for all optimization challenges, loss-based sampling initialization represents a practical tool grounded in theoretical understanding. The simplicity of the approach - requiring no architectural changes or complex mathematics - makes it immediately applicable to existing workflows.

This work suggests that effective neural network training may depend less on sophisticated optimization algorithms and more on simply starting closer to where we want to end up. Sometimes the most profound insights are the simplest ones.

All experimental code and data are available at https://github.com/alanoursland/tr_xor for reproduction and extension of these results.

This article was drafted by AI based on original research, ideas and discussion.

