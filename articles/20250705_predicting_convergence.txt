Predicting Neural Network Convergence Time from Parameter State: An Empirical Investigation

Introduction
This work explores whether neural network convergence time can be predicted from the current parameter state during training. Using a minimal single-neuron model with absolute value activation (y = |Wx + b|) trained on centered XOR data, we investigate the relationship between parameter configurations and epochs required for convergence. Our preliminary findings suggest a surprisingly simple linear relationship between parameter distance and training time, challenging assumptions about optimization complexity.

The complete source code and data are available at https://github.com/alanoursland/tr_xor

Motivation and Approach
The core question motivating this work is whether certain neural network parameter configurations are inherently "slower" to optimize beyond simply being farther from the solution. If such regions exist, identifying them could inform better initialization strategies and provide insights into optimization dynamics.

Our approach exploits the Markov property of vanilla stochastic gradient descent (SGD). Without momentum or adaptive learning rates, the future trajectory depends only on the current parameter state, suggesting that convergence time should be a learnable function of current parameters.

We deliberately chose a minimal model for this investigation:

- Function: y = |Wx + b| where W in R^2, b in R

- Task: Centered XOR classification 

- Optimizer: Vanilla SGD (no momentum)

- Property: Guaranteed convergence due to favorable gradient structure

This model was selected because the absolute value activation creates a well-behaved optimization landscape while still exhibiting non-linear dynamics. The relationship to ReLU networks (since |x| = ReLU(x) + ReLU(-x)) suggests potential broader applicability of findings.

Experimental Design
We generated 100 independent training traces, each run to convergence, producing 3,445 total epochs of data. From these traces, we created 3,345 supervised learning examples where:

- Input: (current_parameters, final_converged_parameters)

- Target: epochs_remaining_to_convergence

This framing transforms the convergence prediction problem into a regression task. Each epoch in each training trace provides one data point, allowing us to learn the mapping from parameter state to convergence time.

Feature Engineering
To capture different aspects of parameter space geometry, we computed 18 features:

1. Distance metrics:

 - L1_distance = |p0_init - p0_final| + |p1_init - p1_final| + |p2_init - p2_final|

 - L2_distance = sqrt((p0_init - p0_final)^2 + (p1_init - p1_final)^2 + (p2_init - p2_final)^2)

 - cosine_distance = 1 - (initial_params · final_params) / (||initial_params|| × ||final_params||)

 - Parameter-wise absolute differences

2. Raw differences: diff_pi = pi_init - pi_final for each parameter

3. Ratios: ratio_pi = pi_init / (pi_final + 1e-8) for each parameter

4. Log transforms: log_diff_pi = log(|pi_init - pi_final| + 1e-8) for each parameter

5. Interaction terms: interaction_pi_pj = diff_pi × diff_pj for all parameter pairs

These features test different hypotheses about the relationship between parameter state and convergence time, from simple distances to multiplicative relationships and parameter coupling effects.

Model Results
We trained 11 different regression models using 10-fold cross-validation:

1. Gradient Boosting: R² = 1.0000

2. Random Forest: R² = 1.0000 

3. RBF Kernel: R² = 0.9992

4. Neural Net (3-layer): R² = 0.9986

5. Neural Net (2-layer): R² = 0.9794

6. Linear Regression: R² = 0.9666

7. Polynomial (degree 2): R² = 0.9466

8. Polynomial (degree 3): R² = 0.9237

9. Log-Linear: R² = 0.9200

10. Exponential: R² = 0.8832

11. Linear Kernel: R² = -105.6795

Key Findings
The most striking result is that simple linear regression achieves 96.66% accuracy in predicting convergence time. This suggests the fundamental relationship is:

epochs_remaining = α × distance_from_solution + β

Several observations support this interpretation:

1. Performance decreases with model complexity. Polynomial models perform worse than linear, suggesting the true relationship is closer to linear than polynomial.

2. Log-linear and exponential models underperform simple linear regression, indicating convergence follows linear rather than exponential decay.

3. Tree-based methods achieve perfect prediction, likely by capturing the remaining 3.34% variance through handling edge cases or discretization effects.

4. Both L1 and L2 distance metrics yield strong linear relationships, consistent with norm equivalence in finite dimensions (||x||_2 ≤ ||x||_1 ≤ sqrt(3)||x||_2 for our 3-parameter model).

Theoretical Interpretation
The linear relationship between distance and convergence time reveals several important properties of the optimization landscape:

1. Uniform convergence rate: The optimization proceeds at constant "speed" in parameter space regardless of location.

2. No gradient pathologies: There are no regions where gradients become vanishingly small, optimization gets stuck in valleys, or parameters require special coordination.

3. Distance determines destiny: The only factor determining training time is distance to solution, not direction or specific parameter values.

This makes sense for the |Wx + b| model because:

- The activation is piecewise linear with consistent gradient magnitude

- There are no vanishing gradients (unlike sigmoid/tanh) or dead neurons (unlike ReLU)

- The V-shaped landscape provides clear gradient signals throughout parameter space

Essentially, the model creates a "gradient highway" with no traffic jams - constant speed from any starting point to the solution.

Implications and Limitations
Our findings suggest that in well-behaved optimization landscapes, convergence time is predictable and systematic. The search for "difficult" parameter regions (beyond simple distance) may be misguided for such models. This has several implications:

1. Initialization strategies should focus on reducing initial distance rather than avoiding mythical "bad" regions.

2. The linear relationship enables simple convergence time estimation from initial parameters.

3. Many perceived difficulties in neural network training may simply result from starting far from good solutions.

However, important limitations must be acknowledged:

1. Results are specific to a minimal model with absolute value activation and vanilla SGD.

2. The centered XOR task may not represent more complex optimization landscapes.

3. ReLU networks can exhibit training pathologies (dead neurons, local minima) not present in our model.

4. Scaling to deep networks introduces additional complexities through layer interactions.

Future Directions
Several natural extensions of this work suggest themselves:

1. Testing whether ReLU variants (y = ReLU(W0x + b0) + ReLU(W1x + b1)) exhibit similar linear relationships when they converge successfully.

2. Investigating whether layer-wise distance metrics predict convergence in deep networks.

3. Implementing the suggested initialization strategy (sampling many initializations and selecting those with low initial loss) to quantify practical training time improvements.

4. Exploring when and how the linear relationship breaks down in more complex models.

Broader Connections
This work connects to several areas of machine learning research:

1. The Markov property of vanilla SGD enables our entire approach - with momentum or adaptive optimizers, past trajectory would matter.

2. Norm equivalence in finite dimensions explains why different distance metrics yield similar results.

3. The linear relationship suggests that effective learning rate (step size relative to distance) may be more fundamental than absolute learning rate.

4. Our findings align with empirical observations that "lucky" initializations train faster, providing a theoretical basis for this phenomenon.

Conclusions
Through systematic empirical investigation, we have shown that convergence time for a minimal neural network model is linearly proportional to parameter distance from the solution. This negative result - failing to find gradient-predictable "slow" regions - is itself informative, revealing the fundamental simplicity of optimization in well-behaved landscapes.

While our specific model (|Wx + b| on centered XOR) is deliberately minimal, the clarity of the linear relationship suggests that similar principles may apply more broadly. The key insight is that in the absence of optimization pathologies, training time is determined by distance traveled rather than local parameter space properties.

This work demonstrates the value of empirical investigation of optimization dynamics using interpretable models. By learning to predict convergence, we gain insights into the nature of the optimization process itself. Future work will explore how these insights extend to more complex architectures and tasks.

All code, data, and analysis scripts are available at https://github.com/alanoursland/tr_xor for reproduction and extension of these results.

This article was drafted by AI based on original research and ideas.

