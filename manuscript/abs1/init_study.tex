% !TeX root = ../main.tex

\section{Initialization Study}
\label{sec:abs1-init}

\subsection*{Experimental Variants}
We test five weight–initialisation schemes—\textsc{tiny}, \textsc{normal},
\textsc{xavier}, \textsc{kaiming}, and \textsc{large}—were evaluated under
identical training conditions (Adam, $lr=0.01$, $50$ seeds, early–stop at
$\mathcal L<10^{-7}$).  The following subsections present classification
accuracy, convergence timing, weight–space movement, and geometric outcomes.

% ------------------------------------------------------------------
\subsection*{Classification Accuracy}

\begin{table}[h]
\centering
\caption{Number of runs (out of 50) achieving each discrete accuracy level
on the centred XOR dataset.  All variants ultimately attain $100\,\%$
accuracy.}
\label{tab:init-accuracy}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Init} & \multicolumn{5}{c}{Accuracy level}\\
\cmidrule(lr){2-6}
 & 0\% & 25\% & 50\% & 75\% & 100\% \\
\midrule
Tiny    & 0 & 0 & 0 & 0 & 50 \\
Normal  & 0 & 0 & 0 & 0 & 50 \\
Xavier  & 0 & 0 & 0 & 0 & 50 \\
Kaiming & 0 & 0 & 0 & 0 & 50 \\
Large   & 0 & 0 & 0 & 0 & 50 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Discussion.}  
Because the analytic optimum is reachable from any orientation, every
initialiser eventually yields perfect XOR classification.  The accuracy table
will become more informative in later chapters where deeper models can stall
at intermediate performance plateaus.

% ------------------------------------------------------------------
\subsection*{Convergence Timing}

\begin{table}[h]
\centering
\caption{Epochs to reach $\mathcal L<10^{-7}$ (percentiles over 50 runs).}
\label{tab:init-epochs}
\begin{tabular}{lccccc}
\toprule
Init & 0\,\% & 25\,\% & 50\,\% & 75\,\% & 100\,\% \\
\midrule
Tiny    & 75  & 141 & 147 & 154 & 166 \\
Normal  & 76  & 127 & 146 & 164 & 297 \\
Xavier  & 62  & 122 & 151 & 234 & 449 \\
Kaiming & 61  & 139 & 198 & 266 & 548 \\
Large   & 154 & 527 & 671 & 878 & 1670 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Discussion.}  
Convergence time grows monotonically with initial weight scale.  Tiny and
Normal inits start close to the optimal norm and need mainly a small rotation,
whereas Large must shrink by orders of magnitude before fine–tuning—the
principal driver of its median $671$-epoch runtime.

% ------------------------------------------------------------------
\subsection*{Weight Orientation and Scale}

\begin{table}[h]
\centering
\caption{Median angle (°) between initial and final weights and median
norm ratio $\lVert W_{\text{init}}\rVert / \lVert W_{\text{final}}\rVert$.}
\label{tab:init-angle-norm}
\begin{tabular}{lcc}
\toprule
Init & Angle (median) & Norm ratio (median) \\
\midrule
Tiny    & 22.0 & 0.16 \\
Normal  & 23.2 & 0.81 \\
Xavier  & 22.1 & 1.33 \\
Kaiming & 22.0 & 1.63 \\
Large   & 22.0 & 6.54 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Discussion.}  
Median angle is remarkably stable ($\sim22^\circ$) across inits, indicating
that orientation correction happens early and is largely independent of scale.
Norm ratio, by contrast, expands linearly with $\sigma$; for the Large init,
magnitude dominates the optimisation path, masking any subtle angle effects.
A deeper investigation will be required to untangle orientation from scale in
more complex settings.

% ------------------------------------------------------------------
\subsection*{Final Loss Distribution}

\begin{table}[h]
\centering
\caption{Mean final loss and range across runs.}
\label{tab:init-loss}
\begin{tabular}{lccc}
\toprule
Init & Mean & Min & Max \\
\midrule
Tiny    & $1.32\times10^{-7}$ & $1.6\times10^{-8}$ & $3.0\times10^{-7}$ \\
Normal  & $7.81\times10^{-8}$ & $2.6\times10^{-9}$ & $7.6\times10^{-7}$ \\
Xavier  & $1.11\times10^{-7}$ & $2.4\times10^{-11}$ & $2.0\times10^{-7}$ \\
Kaiming & $7.06\times10^{-8}$ & $3.1\times10^{-10}$ & $1.0\times10^{-6}$ \\
Large   & $7.37\times10^{-8}$ & $1.4\times10^{-8}$ & $8.4\times10^{-8}$ \\
\bottomrule
\end{tabular}
\end{table}

Even the slowest runs terminate near machine precision, underscoring the
well-conditioned nature of the loss landscape once the weight norm is
corrected.

% ------------------------------------------------------------------
\subsection*{Hyperplane Geometry}
\label{sec:init-geometry}

The analytic optimum (Section~\ref{sec:abs1-model-data}) predicts that learning
should place the prototype surface on the two \textbf{False} points and leave
the \textbf{True} points at distance $\sqrt2$.  
To verify that outcome quantitatively we perform two post–training cluster
analyses:

\begin{enumerate}[label=(G\arabic*)]
    \item \textbf{Distance–pattern clustering} –  
        For each run we compute two summary numbers:  
        \(d_{\text{False}}\), the mean Euclidean distance of the two
        class-False points to the learned surface, and  
        \(d_{\text{True}}\), the mean distance of the two class-True points.
        The pair \((d_{\text{False}}, d_{\text{True}})\) characterises the
        run’s \emph{distance pattern}.  
        We then apply DBSCAN in this two-dimensional space to group runs whose
        distance patterns are (up to numerical tolerance) identical.
    \item \textbf{Weight–space clustering} –  
          We apply DBSCAN to the parameter vectors $(w_1,w_2,b)$ of the 50
          runs.  Each resulting group is a \emph{weight cluster}.  If two
          clusters are identified they must be sign–symmetric because the
          centred XOR data are symmetric under $(w,b)\!\mapsto\!(-w,-b)$.
\end{enumerate}

\paragraph{Results.}
Table~\ref{tab:init-geometry} summarises the outcome for all five
initialisers.

\begin{table}[h]
\centering
\caption{Geometry of the learned prototype surface across 50 runs per
initialiser.  Distance values are mean$\pm$std.  The entry
“2 (27/23)” means two weight clusters containing 27 and 23 runs, respectively.}
\label{tab:init-geometry}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Init} &
\multicolumn{2}{c}{Distance to surface} &
\multirow{2}{*}{\#\,distance clusters} &
\multirow{2}{*}{\#\,weight clusters} \\
\cmidrule(lr){2-3}
 & Class 0 & Class 1 & & \\
\midrule
Tiny    & $0.00\pm0.00$ & $1.41\pm0.00$ & 1 & 2 (27/23) \\
Normal  & $0.00\pm0.00$ & $1.41\pm0.00$ & 1 & 2 (30/20) \\
Xavier  & $0.00\pm0.00$ & $1.41\pm0.00$ & 1 & 2 (27/23) \\
Kaiming & $0.00\pm0.00$ & $1.41\pm0.00$ & 1 & 2 (27/23) \\
Large   & $0.00\pm0.00$ & $1.41\pm0.00$ & 1 & 2 (27/23) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpretation.}
\begin{itemize}
    \item \textbf{Distance pattern clusters.}  
          Every initialiser produces a \emph{single} distance pattern:
          class-0 points lie exactly on the surface
          ($d_{\text{False}}\!=\!0$) and class-1 points lie exactly
          $\sqrt2$ away.  This matches the analytic optimum and confirms that
          the neuron encodes class identity via the \emph{location} of its
          zero-level set rather than by maximising activation magnitude.
    \item \textbf{Weight clusters.}  
          DBSCAN consistently finds two parameter clusters whose centroids are
          sign flips of one another
          \(\bigl(\tfrac12,-\tfrac12,0\bigr)\) and
          \(\bigl(-\tfrac12,\tfrac12,0\bigr)\).
          Which cluster a run lands in is determined by the sign of its
          initial weights, not by their scale, explaining why the counts
          differ slightly across initialisers.
\end{itemize}

\paragraph{Why this matters for Prototype Surface Learning.}
Prototype Surface Learning asserts that a neuron represents a class by the
surface where its pre-activation is zero.  
The empirical geometry here shows exactly that behaviour: the learned
hyperplane \emph{anchors} to the negative class and positions the positive
class on parallel offsets.  
Because the positive class never attains the reference value of zero, its
parallel surfaces are implicit—but fully determined—once the prototype
surface is in place.

In summary, the geometric analysis complements the convergence results: weight
scale controls how quickly the optimiser reaches the optimum, whereas the
final surface location—and thus the class representation—remains invariant
across all initialisation schemes.

% ------------------------------------------------------------------
\subsection*{Conclusion}

Weight scale sets the pace; geometry remains invariant.  Tiny and Normal
inits converge fastest because they start near the optimal norm; Large
requires the optimiser to shrink weights by an order of magnitude before fine
tuning.  Yet all initialisers reach the same two mirror solutions, achieve
$100\,\%$ accuracy, and reproduce the analytically optimal prototype surface.
Future chapters will test whether this separation of \emph{speed} (scale) and
\emph{destination} (geometry) generalises to deeper networks and more complex
datasets.


