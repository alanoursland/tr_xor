% !TeX root = ../main.tex
\section{Optimizer Study}
\label{sec:abs1-optim}

% ------------------------------------------------------------------

\subsection*{Study Motivation}

The \textsc{abs1} architecture admits a closed-form optimum (Section~\ref{sec:abs1-model-data}), creating a unique opportunity to study optimizer behavior when the theoretical ideal is known. Once the gradient points exactly toward that optimum, a single "perfect" step can solve the problem. This section investigates how different optimizers approach this theoretical ideal and what their behavior reveals about optimization dynamics more generally.

The loss surface for this model is locally \emph{exactly quadratic} once the sign pattern of $w^{\mathsf T}x+b$ is fixed, enabling a direct mathematical connection between first-order methods and Newton's method. Taking derivatives of the MSE loss \eqref{eq:mse-loss} for a fixed sign pattern gives:
\[
   \nabla\! \mathcal L(w,b) \;=\; 2\begin{bmatrix}w_1\\ w_2\\ b\end{bmatrix},
   \qquad
   \nabla^2\! \mathcal L(w,b) \;=\; 2I_{3\times3}.
\]
The constant, isotropic Hessian $H = 2I$ means Newton's method proposes the update:
\[
   \Delta\theta_{\text{Newt}} = -H^{-1}\nabla\! \mathcal L = -\tfrac12\,\nabla\! \mathcal L.
\]
Plain SGD with learning rate $\eta$ performs $\Delta\theta_{\text{SGD}} = -\eta\,\nabla\! \mathcal L$. Setting $\eta=\tfrac12$ makes $\Delta\theta_{\text{SGD}} = \Delta\theta_{\text{Newt}}$, meaning each SGD step with $\eta=0.5$ coincides \emph{exactly} with a Newton step.

This mathematical equivalence allows us to test two key questions:
\begin{itemize}
   \item How close does SGD with the theoretically optimal learning rate get to ideal single-step convergence?
   \item How does Adam's adaptive-moment strategy interact with this well-conditioned optimization landscape?
\end{itemize}

Beyond the immediate practical insights, this experiment serves as a controlled study of pure optimizer characteristics. Since the destination is mathematically determined, any differences in behavior isolate the effects of momentum, adaptive scaling, and step-size selection—knowledge that will prove valuable when tackling more complex architectures where Newton steps are unavailable.

% ------------------------------------------------------------------

\subsection*{Study Design}

\paragraph{Model and Data}
All experiments use the single absolute-value neuron architecture $\hat{y}(x) = |w^{\mathsf{T}}x + b|$ on the centered XOR dataset with MSE loss, maintaining consistency with the initialization study (Section~\ref{sec:abs1-init}).

\paragraph{Optimizer Variants}
We test three optimizer configurations, each trained on 50 independent runs with Kaiming normal initialization ($\mathcal{N}(0, 2/n_{\text{in}})$) and early stopping at $\mathcal{L} < 10^{-7}$:

\begin{enumerate}
   \item \textbf{SGD, lr = 0.50} -- Theoretically optimal learning rate that equals Newton steps
   \item \textbf{Adam, lr = 0.01} -- Standard setting from the initialization study for comparison
   \item \textbf{Adam, lr = 0.50} -- High-gain Adam to contrast with optimal SGD behavior
\end{enumerate}

The Adam variants use default momentum parameters ($\beta_1=0.9$, $\beta_2=0.99$) to isolate the effects of learning rate scaling versus the adaptive moment estimation strategy.

\paragraph{Experimental Hypothesis}
Based on the mathematical equivalence derived above, we predict that SGD with $\eta=0.5$ should converge in essentially one substantive parameter update (logged as 2 epochs due to our training loop structure). Adam optimizers should reach the same geometric solution but via different trajectories: Adam(0.01) through many small steps, and Adam(0.5) through initial overshooting followed by momentum-damped oscillations.

% ------------------------------------------------------------------

\subsection*{Success Metrics}

\begin{table}[ht]
\centering
\caption{Classification accuracy across optimizer variants (50 runs each). All optimizers achieve perfect XOR classification.}
\label{tab:abs1-opt-accuracy}
\begin{tabular}{lc}
\toprule
Optimizer & Success Rate \\
\midrule
SGD, lr = 0.50 & 50/50 (100\%) \\
Adam, lr = 0.01 & 50/50 (100\%) \\
Adam, lr = 0.50 & 50/50 (100\%) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{Final loss statistics across optimizer variants.}
\label{tab:abs1-opt-loss}
\begin{tabular}{lccc}
\toprule
Optimizer & Mean & Min & Max \\
\midrule
SGD, lr = 0.50 & $0.00 \times 10^{0}$ & $0.00 \times 10^{0}$ & $0.00 \times 10^{0}$ \\
Adam, lr = 0.01 & $7.06 \times 10^{-8}$ & $3.08 \times 10^{-10}$ & $1.00 \times 10^{-6}$ \\
Adam, lr = 0.50 & $1.90 \times 10^{-6}$ & $2.11 \times 10^{-8}$ & $1.75 \times 10^{-5}$ \\
\bottomrule
\end{tabular}
\end{table}

All optimizer variants achieve universal classification success, consistent with the initialization study results. The robust accuracy across optimizers confirms that the single absolute-value architecture eliminates convergence failures regardless of the optimization strategy employed.

% ------------------------------------------------------------------

\subsection*{Learning Dynamics}

\begin{table}[ht]
\centering
\caption{Epochs to reach $\mathcal{L}\le10^{-7}$ (percentiles over 50 runs).}
\label{tab:abs1-opt-conv}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Optimiser} &
\multicolumn{5}{c}{Epoch percentile} \\
\cmidrule(lr){2-6}
& 0\,\% & 25\,\% & 50\,\% & 75\,\% & 100\,\% \\
\midrule
SGD,  0.50 & 2   & 2   & 2   & 2   & 2   \\ 
Adam, 0.01 & 61  & 139 & 198 & 266 & 548 \\
Adam, 0.50 & 94  & 118 & 126 & 137 & 154 \\
\bottomrule
\end{tabular}
\end{table}

The convergence timing reveals three distinct optimization regimes, spanning nearly two orders of magnitude in training time. SGD with the theoretically optimal learning rate demonstrates the predicted Newton-step behavior, while the Adam variants illustrate different aspects of adaptive optimization dynamics.

\paragraph{SGD(0.50): Near-Instantaneous Convergence}
SGD with $\eta=0.5$ achieves the theoretical ideal, converging in exactly 2 epochs across all runs. This uniform timing reflects the Newton-step equivalence: the first epoch applies the optimal parameter update, bringing the loss nearly to zero, while the second epoch applies a numerically tiny correction that triggers the stopping criterion. The complete independence from initial geometry—evident in the raw data where all angle and norm ranges yield identical 2.0-epoch convergence—confirms that the optimizer makes the perfect step regardless of starting conditions.

\paragraph{Adam(0.01): Gradual Convergence}
The standard Adam configuration exhibits the expected behavior for conservative learning rates. With a base rate of $0.01$, the effective step size remains too small for rapid convergence, requiring hundreds of epochs to reach the optimum. The convergence timing shows sensitivity to initialization geometry similar to the patterns observed in the initialization study, with norm ratios correlating with training duration (124 epochs for small adjustments vs. 448 epochs for large magnitude changes).

\paragraph{Adam(0.50): Momentum-Induced Oscillations}
High-gain Adam demonstrates the interaction between adaptive learning and momentum accumulation. Despite the Newton-optimal base rate, the momentum terms ($\beta_1=0.9$, $\beta_2=0.99$) cause overshooting and subsequent oscillations around the optimum. The median 126-epoch convergence reflects this oscillatory decay, contrasting sharply with SGD's direct approach using the same learning rate. The tighter convergence distribution (94-154 epochs) compared to Adam(0.01) shows that the larger step size dominates over initialization effects.

\paragraph{Geometric Consistency}
All three optimizers achieve 100\% XOR accuracy and reproduce the same two sign-symmetric prototype surfaces reported in Section~\ref{sec:abs1-init}. The hyperplane clustering analysis reveals identical distance patterns (Class 0: 0.00±0.00, Class 1: 1.41±0.00) and weight clusters (centroids at $(\pm0.5, \mp0.5, 0)$) across all optimization strategies. This geometric invariance demonstrates that optimizer choice affects \emph{when} the solution is reached, not \emph{what} is learned, reinforcing the fundamental separation between optimization dynamics and learned representations.

% ------------------------------------------------------------------

\subsection*{Study Discussion}

This experiment provides rare empirical validation of optimization theory under controlled conditions. With the analytical optimum known and the loss surface exactly quadratic, we can isolate pure optimizer effects and test theoretical predictions directly.

\paragraph{Theoretical Validation}
SGD with $\eta=0.5$ achieves the theoretical ideal, confirming that the Newton-step equivalence derived in the motivation holds empirically. The universal 2-epoch convergence across all initialization geometries demonstrates that when the Hessian is constant and isotropic ($H = 2I$), a single properly-scaled gradient step suffices for optimization. This validates both the mathematical analysis and the practical value of leveraging problem structure when available.

The loss evaluation timing explains the "2-epoch phenomenon": the first epoch applies the Newton-sized parameter update, bringing the loss nearly to zero but just above the $10^{-7}$ threshold, while the second epoch applies a numerically tiny correction that triggers early stopping. This technical detail highlights how training loop implementation can obscure the underlying optimization dynamics.

\paragraph{Adaptive Optimization Limitations}
The Adam variants reveal how adaptive methods can introduce unnecessary complexity for well-conditioned problems. Adam(0.01) converges slowly because the base learning rate is simply too conservative, requiring hundreds of small steps to traverse the same distance SGD covers in one. Adam(0.50) demonstrates the momentum interaction problem: despite using the optimal base rate, the accumulated momentum ($\beta_1=0.9$, $\beta_2=0.99$) causes overshooting and oscillatory decay that extends convergence to over 100 epochs.

This illustrates a fundamental limitation of adaptive methods: they optimize for robustness across diverse loss landscapes at the cost of efficiency on well-behaved surfaces. When problem structure is known and exploitable, simpler methods can dramatically outperform sophisticated alternatives.

\paragraph{Speed and Content Separation}
The geometric analysis confirms that optimization choice affects the trajectory but not the destination. All optimizers converge to identical prototype surface structures, with the same distance patterns (Class 0: 0.00±0.00, Class 1: 1.41±0.00) and mirror-symmetric weight clusters. This reinforces the fundamental finding from the initialization study: learned representations emerge from the problem structure and model architecture, not from optimization dynamics.

\paragraph{Implications for Complex Models}
While this "frictionless" optimization problem represents an idealized case, the insights inform practical deep learning. The study demonstrates the value of theoretical analysis for algorithm selection and highlights scenarios where simpler optimizers may outperform adaptive methods. For subsequent experiments with multi-neuron architectures, we will use Adam(0.01) as a reasonable default that balances robustness with computational efficiency, informed by this understanding of its behavior characteristics.

The controlled nature of this experiment—with known optimal solutions and exact loss surface properties—provides a rare opportunity to validate optimization theory empirically. As we transition to more complex models where such analytical tractability is lost, these baseline insights about the relationship between optimization dynamics and learned representations will prove invaluable for interpreting emergent behaviors.