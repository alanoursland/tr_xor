% !TeX root = ../main.tex
\section{Conclusions}
\label{sec:abs1-conclusions}

\subsection*{Empirical Geometry}

Table~\ref{tab:abs1-init-geometry} (p.~\pageref{tab:abs1-init-geometry}) and
Figure~\ref{fig:abs1-hyperplanes} show that, regardless of weight scale or
optimiser, training always produces the same geometric configuration:

\begin{enumerate}[label=(\alph*)]
    \item A single \textbf{distance pattern}: class-False points lie \emph{on}
          the learned surface, class-True points sit exactly
          $\sqrt2$ away.
    \item Two \textbf{weight clusters}: parameter vectors group into the
          sign-symmetric optima
          \(\bigl(\tfrac12,-\tfrac12,0\bigr)\) and
          \(\bigl(-\tfrac12,\tfrac12,0\bigr)\).
\end{enumerate}

These findings match the analytic solution from
Section~\ref{sec:abs1-model-data} and visualise the Prototype-Surface Learning
principle: the neuron encodes a class by \emph{where} its pre-activation
vanishes, not by the magnitude of its positive outputs.

\subsection*{Key Takeaways}

\begin{itemize}
    \item \textbf{Surface before scale}  
          All runs learn the correct surface; weight scale only governs how
          \emph{fast} they arrive.
    \item \textbf{SGD(0.5) $\boldsymbol{\approx}$ Newton}  
          A constant step equal to the inverse Hessian eigenvalue solves the
          problem in essentially one update; adaptive methods converge more
          slowly but to the same geometry.
    \item \textbf{Invariant representation}  
          Whether initial variance is $0.1$ or $4.0$, the prototype surface
          always intersects the negative class and offsets the positive class
          by $\sqrt2$—underscoring that representation depends on surface
          location, not weight magnitude.
\end{itemize}

\subsection*{Open Questions}

\begin{itemize}
    \item \textbf{Predicting convergence}  
          Our results hint at a correlation between geometric displacement
          (e.g.\ norm ratio or distance to the prototype surface) and the
          number of epochs to convergence.  Can we define an invariant metric
          on the \emph{initial-final} state pair that reliably predicts
          training time across initialisers and optimisers?

    \item \textbf{Multi-output extension}  
          The Abs-XOR neuron uses a single scalar output to indicate class
          membership.  What geometric behaviour emerges if we switch to a
          one-hot output vector—one distance channel per class—instead of a
          single distance-to-surface scalar?

    \item \textbf{From $\lvert\cdot\rvert$ to ReLU + ReLU}  
          The next chapter decomposes the absolute value into two half-space
          ReLUs:  
          \[
              \lvert w\!\cdot\!x + b\rvert
              \;=\;
              \operatorname{ReLU}(w_0\!\cdot\!x + b_0)
              \;+\;
              \operatorname{ReLU}(w_1\!\cdot\!x + b_1).
          \]
          Will the clean separation between \emph{optimisation speed} (set by
          scale) and \emph{learned geometry} (set by surface location) persist
          once we learn each half of the Abs function independently?
\end{itemize}

These questions point beyond our idealized toy problem and the single 
neuron Abs model and set the stage for the progressively richer systems 
examined in the next chapters.
