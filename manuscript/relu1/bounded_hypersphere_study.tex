% !TeX root = ../main.tex
\section{Bounded Hypersphere Initialization Study}
\label{sec:relu1-bounded-hypersphere}

% ------------------------------------------------------------------

\subsection*{Study Motivation}

The rejection sampling approach demonstrated in the previous section achieves excellent results but relies on a computationally inefficient process: repeatedly discarding random initializations until finding configurations without dead data. While effective, this method lacks theoretical elegance, depending on chance rather than design to satisfy the required constraints. For basic reinitialization, approximately 60\% of random draws are rejected, while margin requirements can reject over 90\% of candidates before finding suitable configurations.

This study explores a constructive alternative that directly generates initializations satisfying our requirements through geometric design. Rather than filtering random configurations, Bounded Hypersphere (BHS) initialization places hyperplanes according to principled geometric constraints that guarantee all data points begin with positive activations. This approach represents a shift from probabilistic search to deterministic construction.

The BHS method positions each hyperplane tangent to a hypersphere of radius $r=1.4$ centered on the data mean, with normals oriented inward. This geometry ensures that all four XOR points lie on the positive side of $\operatorname{ReLU}(w \cdot x + b)$ for every hidden unit, providing non-zero gradients from the first training step. The hypersphere radius is chosen to encompass all data points while maintaining reasonable hyperplane distances.

Beyond practical performance, BHS initialization offers theoretical appeal as a more principled solution to the dead data problem. By replacing stochastic search with deterministic construction, it provides a cleaner framework for analyzing how initialization geometry affects learning dynamics. The method's explicit geometric constraints enable precise characterization of starting configurations and their relationship to final solutions.

% ------------------------------------------------------------------

\subsection*{Study Design}

\paragraph{Experimental Configuration}
The BHS study maintains the same 2-ReLU architecture used throughout this investigation (Linear(2→2) → ReLU → Sum) to enable direct comparison with baseline and rejection sampling approaches. The key innovation lies in the initialization procedure, which combines Kaiming weight initialization with geometric bias construction to create hyperplanes tangent to a bounding hypersphere. The hypersphere radius of $r=1.4$ is chosen to encompass all four XOR points while maintaining reasonable distances between hyperplanes and data, balancing the competing requirements of guaranteed activation and avoiding excessive initial distances.

\paragraph{BHS Implementation}
The initialization proceeds in two phases. First, weights are initialized using standard Kaiming normal distribution, ensuring appropriate variance scaling for ReLU networks. Second, biases are geometrically constructed to position each hyperplane tangent to the hypersphere. For each hidden unit with weight vector $w$, the algorithm computes a point on the hypersphere as $p = -w \cdot r / ||w||$ and sets the bias $b = -w \cdot p$ to achieve tangency. This construction guarantees that all hyperplanes have their normal vectors pointing inward toward the hypersphere center, ensuring all data points begin on the positive side of every ReLU unit.

\paragraph{Hybrid Verification}
While BHS construction theoretically guarantees positive activations, the implementation includes a verification step requiring all points to achieve activation values above 0.3. This margin requirement, inherited from the rejection sampling study, provides additional robustness against numerical edge cases and ensures fair comparison with margin-based approaches. If verification fails, the network is reinitialized up to 100 attempts, though in practice BHS construction rarely requires multiple attempts.

\paragraph{Training Protocol}
Training parameters remain identical to baseline configurations except for an extended epoch limit of 2000 (versus 800 baseline) to accommodate potentially slower convergence from the constrained initialization. The Adam optimizer uses learning rate 0.01 with momentum parameters (0.9, 0.99), and training employs MSE loss with early stopping at $10^{-7}$. This consistency in training protocol ensures that performance differences can be attributed solely to initialization strategy rather than optimization parameters.

\paragraph{Analysis Framework}
The experimental design employs 50 independent runs to characterize BHS performance across multiple metrics. Success rate analysis quantifies the improvement over the 48\% baseline, while convergence timing reveals the computational cost of geometric constraints. Geometric analysis examines distance patterns, weight clustering, and mirror symmetry detection to assess solution quality. Particular attention is given to failure mode characterization, as the geometric constraints may introduce systematic vulnerabilities distinct from those observed in random initialization. All analyses use identical procedures to previous studies, enabling direct statistical comparison of initialization strategies.

% ------------------------------------------------------------------

\subsection*{Success Metrics}

\begin{table}[ht]
\centering
\caption{Classification accuracy comparison across initialization strategies.}
\label{tab:relu1-bhs-success}
\begin{tabular}{lccc}
\toprule
Variant & Success Rate & Performance vs Baseline & Runs \\
\midrule
ReLU (Baseline) & 24/50 (48\%) & -- & 50 \\
Reinit (basic) & 46/50 (92\%) & +92\% relative & 50 \\
Reinit + margin 0.3 & 496/500 (99.2\%) & +107\% relative & 500 \\
BHS ($r=1.4$) & 36/50 (72\%) & +50\% relative & 50 \\
\bottomrule
\end{tabular}
\end{table}

The BHS initialization achieves a 72\% success rate (36/50 runs), representing a substantial improvement over the 48\% baseline but falling short of rejection sampling's 92\% performance. This intermediate success rate reflects the trade-offs inherent in constructive initialization: while BHS guarantees active gradients from all data points, its geometric constraints introduce new failure modes not present in rejection sampling approaches.

\begin{table}[ht]
\centering
\caption{Accuracy distribution for BHS initialization.}
\label{tab:relu1-bhs-accuracy-dist}
\begin{tabular}{lcccc}
\toprule
Accuracy & 100\% & 75\% & 50\% & 25\% \\
\midrule
Runs & 36 & 2 & 10 & 2 \\
Percentage & 72\% & 4\% & 20\% & 4\% \\
\bottomrule
\end{tabular}
\end{table}

The accuracy distribution reveals a stark bimodal pattern. Beyond the 36 successful runs achieving perfect classification, 14 runs failed with degraded accuracy: 10 runs (20\%) achieved only 50\% accuracy, while 4 runs (8\%) performed worse. This distribution differs markedly from rejection sampling, where failures consistently reached 75\% accuracy. The prevalence of 50\% accuracy failures suggests that BHS initialization can lead to degenerate solutions where the network effectively learns only one hyperplane or produces overlapping decision boundaries.

Dead data analysis confirms that BHS successfully eliminates the primary failure mode identified in the baseline study. All 50 runs maintained "no dead inputs" throughout training, validating that the geometric construction achieves its design goal of ensuring positive activations for all data points. However, this complete elimination of dead data proves necessary but not sufficient for reliable learning, as evidenced by the 28\% failure rate despite universal gradient availability.

The performance hierarchy across initialization strategies—baseline (48\%) < BHS (72\%) < rejection basic (92\%) < rejection margin (99.2\%)—demonstrates that while constructive approaches can improve upon random initialization, they may introduce geometric rigidities that limit their effectiveness compared to selective sampling methods.

% ------------------------------------------------------------------

\subsection*{Learning Dynamics}

\begin{table}[ht]
\centering
\caption{Convergence timing for successful runs (100\% accuracy only, epochs to MSE < $10^{-7}$).}
\label{tab:relu1-bhs-timing}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Variant} &
\multicolumn{5}{c}{Epoch percentile} & \multirow{2}{*}{Count} \\
\cmidrule(lr){2-6}
& 0\,\% & 25\,\% & 50\,\% & 75\,\% & 100\,\% & \\
\midrule
ReLU (Baseline) & 53 & 126 & 190 & 251 & 336 & 24/50 \\
BHS ($r=1.4$) & 221 & 517 & 616 & 713 & 895 & 36/50 \\
\bottomrule
\end{tabular}
\end{table}

BHS initialization dramatically slows convergence compared to the baseline, with median training time increasing from 190 to 616 epochs—a factor of $3.2\times$. This slowdown reflects the fundamental geometric challenge imposed by hypersphere initialization: hyperplanes begin at maximum distance from the data and must gradually shrink inward to carve out appropriate decision regions. The fastest BHS run (221 epochs) takes longer than the median baseline run, indicating that the convergence penalty is systematic rather than variable.

The convergence distribution shows remarkable consistency, with the interquartile range (517-713 epochs) remaining proportionally similar to the baseline (126-251 epochs). This consistency suggests that once networks escape the initial geometric constraints, they follow predictable optimization trajectories toward the mirror-symmetric solutions. The extended training requirement appears to be a fixed cost of starting from the hypersphere boundary rather than a sign of optimization instability.

The trade-off between initialization quality and convergence speed distinguishes BHS from rejection sampling approaches. While rejection sampling achieves both higher success rates (92\%) and faster convergence (151 epochs median), BHS accepts slower optimization in exchange for guaranteed geometric properties. This $3\times$ slowdown represents the computational price of constructive initialization, where geometric elegance comes at the cost of additional optimization work to reach the same prototype surface solutions discovered more quickly from random starting points.

% ------------------------------------------------------------------

\subsection*{Geometric Analysis}

The geometric analysis reveals that BHS initialization produces remarkably pristine solutions when successful, achieving unprecedented consistency in discovered patterns while explaining the specific failure modes that limit overall success rates.

\begin{table}[ht]
\centering
\caption{Distance patterns and clustering across initialization strategies.}
\label{tab:relu1-bhs-geometry}
\begin{tabular}{lcccc}
\toprule
Variant & Distance Patterns & Class 0 Distance & Class 1 Distance & Total Hyperplanes \\
\midrule
ReLU (Baseline) & 1 & $0.32 \pm 0.21$ & $1.37 \pm 0.05$ & 48 \\
Reinit (basic) & 1 & $0.32 \pm 0.19$ & $1.37 \pm 0.04$ & 92 \\
BHS ($r=1.4$) & 1 & $0.01 \pm 0.01$ & $1.41 \pm 0.00$ & 72 \\
\bottomrule
\end{tabular}
\end{table}

Distance pattern analysis demonstrates exceptional geometric precision in BHS solutions. All 72 hyperplanes from successful runs converge to a single distance pattern with Class 0 points essentially on the hyperplane ($0.01 \pm 0.01$) and Class 1 points at exactly the prototype surface prediction ($1.41 \pm 0.00$, matching $\sqrt{2}$). The near-zero Class 0 distance represents the limiting case of prototype surface geometry, where False class points lie directly on the decision boundary. This extreme precision, with standard deviations approaching machine epsilon, indicates that BHS initialization channels networks toward the most geometrically pure form of the theoretical solution.

\begin{table}[ht]
\centering
\caption{Weight space clustering and mirror symmetry detection.}
\label{tab:relu1-bhs-weights}
\begin{tabular}{lcccc}
\toprule
Variant & Weight Clusters & Cluster Centroids & Mirror Detection & Perfect Mirrors \\
\midrule
ReLU (Baseline) & 9 & Various & 16/50 (32\%) & 3 \\
Reinit (basic) & 7 & Various & 39/50 (78\%) & 13 \\
BHS ($r=1.4$) & 2 & $\pm[0.502, -0.502]$ & 38/38 (100\%) & 38 \\
\bottomrule
\end{tabular}
\end{table}

Weight clustering analysis reveals unprecedented solution consistency. DBSCAN identifies exactly two clusters with centroids at $\pm[0.502, -0.502]$, representing perfect sign-symmetric pairs. Every successful run contains exactly one weight from each cluster, forming mirror pairs with cosine similarity of $-0.99999 \pm 0.00002$. This 100\% mirror detection rate with near-perfect symmetry (mean error $|cos + 1| = 0.00001$) surpasses all other initialization methods. The weight values correspond to normalized vectors pointing toward XOR class centers, confirming that BHS guides networks to discover the theoretically optimal orientation.

The geometric perfection of successful BHS runs contrasts sharply with the failure modes. Analysis of initial angles shows no significant difference between successful units ($43.08^\circ \pm 23.77^\circ$) and failed units ($43.97^\circ \pm 26.13^\circ$), indicating that failures do not arise from perpendicular initialization as hypothesized. Instead, the uniform hypersphere placement appears to create an "orientation trap" where certain initial configurations, despite having active gradients, cannot escape to find proper mirror-symmetric solutions. The 50\% accuracy failures suggest these trapped networks converge to degenerate solutions where both hyperplanes learn similar orientations rather than the required opposing configuration.

This geometric analysis establishes BHS as producing the highest quality solutions when successful, achieving theoretical optimality in weight symmetry and distance patterns. However, the geometric constraints that ensure this quality also create failure modes absent in more flexible initialization schemes, explaining the intermediate 72\% success rate.

% ------------------------------------------------------------------

\subsection*{Study Discussion}

The Bounded Hypersphere initialization study demonstrates both the promise and perils of constructive approaches to neural network initialization. By geometrically positioning hyperplanes tangent to a hypersphere, BHS successfully eliminates dead data by construction—all 50 runs maintained active gradients throughout training, validating that principled geometric design can reliably solve gradient flow problems. The 72\% success rate represents a substantial improvement over the 48\% baseline, confirming that addressing dead data through construction rather than rejection provides meaningful benefits.

The most striking finding is the geometric perfection achieved by successful BHS runs. While rejection sampling produces varied solutions with 78\% mirror symmetry detection, BHS achieves 100\% perfect mirror symmetry in all successful runs, with cosine similarities of $-0.99999 \pm 0.00002$. The distance patterns show similar precision: Class 0 points lie essentially on the hyperplanes ($0.01 \pm 0.01$), while Class 1 points sit at exactly the theoretical prediction ($1.41 \pm 0.00$). Weight clustering reveals only two perfectly opposing clusters at $\pm[0.502, -0.502]$, representing the ideal solution geometry. This unprecedented consistency suggests that BHS, when successful, reliably guides networks to global optima rather than merely acceptable local solutions.

The $3.2\times$ slower convergence (616 vs 190 epochs median) has a clear geometric explanation. BHS positions all hyperplanes at radius 1.4 from the origin, while optimal XOR solutions pass through the origin. This guarantees that every hyperplane must travel approximately 1.4 units inward during training, creating a uniform distance penalty absent in random initialization where some hyperplanes may start near their optimal positions by chance. This geometric displacement fully accounts for the additional optimization work required.

The extended optimization journey created by starting far from the solution appears to act as a double-edged sword. The additional distance may provide beneficial "orientation discovery" time, allowing hyperplanes to find optimal mirror-symmetric configurations before committing to local basins. This could explain why successful runs achieve such perfect geometry—the slow inward progression enables careful coordination between hyperplanes. However, the same freedom creates opportunities for failure, as hyperplanes can drift into incompatible configurations during their extended journey. The 50\% accuracy failures, where networks learn degenerate overlapping boundaries, suggest that some trajectories lead irreversibly away from viable solutions.

The 28\% failure rate, despite guaranteed gradient flow, reveals that BHS creates its own characteristic failure mode: an orientation trap. Unlike dead data failures that prevent learning entirely, BHS failures involve active optimization toward wrong solutions. The uniform outward placement, while solving one problem, introduces geometric rigidities that prevent some configurations from discovering the required mirror symmetry. The initial angle analysis showing no difference between successful and failed runs (both ~43°) indicates these failures arise from subtle trajectory effects rather than obviously bad starting orientations.

The comparison between BHS and rejection sampling illuminates fundamental trade-offs in initialization design. Rejection sampling achieves higher success rates (92\% basic, 99.2\% with margins) by selecting favorable configurations from the natural distribution of random initializations. BHS accepts lower success rates (72\%) in exchange for geometric guarantees and perfect solutions when successful. This trade-off between flexibility and structure suggests that optimal initialization strategies may need to balance multiple objectives rather than optimizing for a single criterion.

These findings offer several insights for initialization design. First, eliminating known failure modes like dead data is necessary but not sufficient for reliable learning—new geometric constraints can introduce new failure modes. Second, starting far from the solution can paradoxically improve final solution quality by providing exploration time, but this must be balanced against divergence risks. Future work might explore hybrid approaches that combine constructive guarantees with controlled randomness, or adaptive schemes that modulate the hypersphere radius based on problem geometry. The ideal initialization may need to balance multiple factors: gradient flow, distance to optimum, exploration freedom, and basin of attraction alignment.

% ------------------------------------------------------------------
