% !TeX root = ../main.tex
\section{Experimental Framework}
\label{sec:relu1-framework}

This chapter's experimental protocol inherits the core principles of data handling, metric collection, and post-training analysis from the framework defined in Chapter~\textit{Abs1} (Section~\ref{sec:abs1-framework}). This section summarises the key configurations and differences specific to the two-ReLU model.

\subsection*{Model and Training Protocol}
All experiments use the two-ReLU model defined in Section~\ref{sec:relu1-model-data} on the centered XOR dataset. Unless specified otherwise, each variant is trained on 50 or more independent seeds using the Adam optimizer ($\text{lr}=0.01$) and MSE loss.

The baseline runs employ a dual early-stopping criterion, halting if either the MSE drops below \(\varepsilon=10^{-7}\) or the loss fails to improve for 10 consecutive epochs. Specific interventions, such as the runtime monitors, may modify these rules or the total epoch budget.

\subsection*{Experimental Variants}
Unlike the previous chapter, which focused on standard initializers, this chapter evaluates a suite of interventions designed to improve the baseline model's 48\,\% success rate. The primary variants tested include:
\begin{itemize}
    \item \textbf{Activation functions:} Leaky ReLU with various slopes, ELU, and PReLU.
    \item \textbf{Static initialisation schemes:} Re-initialisation based on "live" data (with and without a margin), bounded-hypersphere initialization, and mirror-symmetric initialization.
    \item \textbf{Dynamic runtime interventions:} Online monitors that detect and correct dead data or out-of-bounds weights, and an error-entropy annealing schedule that injects noise to escape local minima.
\end{itemize}

\subsection*{Analysis}
The post-training analyses of convergence, accuracy, and geometry follow the same methods as the previous chapter. Additional diagnostics specific to this model were added, including robust mirror-weight symmetry detection and failure mode analysis.

\begin{enumerate}[label=(A\arabic*)]
    \item \textbf{Binary accuracy} -  
        For each run the model output on every data point is compared to the
        two target values \(\{0,1\}\); a prediction is deemed correct if it
        is \emph{closer} to the true label than to the false one.
        Aggregating over the four inputs yields run-level accuracy, whose
        distribution across 50 runs is then reported.
    \item \textbf{Final-loss distribution} -  
        Mean, variance, and extreme values of the terminating loss; provides
        a stability check beyond the binary accuracy metric.
    \item \textbf{Convergence statistics} -  
        Five-quantile summary (0\,\%, 25\,\%, 50\,\%, 75\,\%, 100\,\%) of
        the number of epochs required to satisfy the stopping criterion
        \(\mathcal{L}<\varepsilon\).
    \item \textbf{Hyperplane geometry} -  
        (i) Distance of each input to the learned prototype surface
        \(f(x)=0\);  
        (ii) clustering of the resulting hyperplanes across runs to detect
        symmetry-related solutions.
    \item \textbf{Mirror-weight symmetry} - 
        Quantifies the geometric alignment of the two hidden neurons by computing the cosine similarity between their weight vectors \((w^{(0)}, w^{(1)})\). This directly tests whether the network learns the opposing-vector solution (\(w^{(1)} \approx -w^{(0)}\)) predicted by the \(|z|=\operatorname{relu}(z)+\operatorname{relu}(-z)\) identity.
    \item \textbf{Failure-angle analysis} - 
        Measures the angle between a run's initial weight vectors and the known optimal orientation (\(w^\star \propto (1,-1)\)). This is used to diagnose the `perpendicular trap' failure mode, where initializations starting near \(90^\circ\) from the optimum are prone to stalling.
    \item \textbf{Dead-data analysis} - 
        Counts the number of input samples \(x_i\) that are inactive for \emph{every} neuron in the layer (\(\operatorname{relu}(w^{(j)\!\top}x_i+b^{(j)})=0\) for all \(j\)). This quantifies the severity of the "dead input" problem and evaluates the effectiveness of interventions designed to ensure gradient flow.
\end{enumerate}

\subsection*{The Importance of Hyperplane Geometry Analysis}
\label{sec:analysis-importance}

The hyperplane geometry analysis is the primary tool used in this research to 
move beyond simple accuracy metrics and directly test the core claims of the 
prototype surface theory. By quantifying the geometric properties of the 
learned neurons, this analysis provides a crucial bridge between the abstract 
theory and the empirical results.

The analysis provides a direct, empirical validation of the theory's central 
mechanism. The consistent finding of near-zero distances between the learned 
hyperplanes and specific data points offers strong evidence that the network 
learns by \textbf{intersecting feature prototypes}. This geometric 
intersection is the signature of a successful prototype recognition. This 
process can also be understood from a representation learning perspective, 
where the linear layer learns a projection into a \textbf{latent space} where 
the data classes become more effectively clustered and separable than in the 
original input space.

Furthermore, by clustering the hyperplanes from hundreds of independent runs, 
the analysis maps the entire landscape of learned solutions. This was critical 
for discovering that successful runs consistently converge to a small set of 
\textbf{symmetric, V-shaped solutions}, revealing a powerful geometric 
"attractor" in the learning dynamics. This approach also highlighted how the 
\textbf{consistency} of these solutions is affected by how constrained the model 
is. The analysis demonstrated how interventions that add constraints—such as 
leaky activations or mirror-initialization—drastically improve geometric 
consistency by mitigating the underconstrained nature of the pure ReLU 
activation, guiding the optimizer to the ideal solution.