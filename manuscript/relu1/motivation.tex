% !TeX root = ../main.tex
\section{Introduction}
\label{sec:relu1-introduction}

The previous chapter showed that a \emph{single} absolute-value unit, \(y=\lvert w^{\mathsf T} x + b\rvert\), can solve the XOR problem almost deterministically. This success is rooted in the mathematical identity \(\lvert z\rvert=\operatorname{relu}(z)+\operatorname{relu}(-z)\), which hard-codes two symmetric half-spaces into the activation itself. In this chapter, we deconstruct this identity to explore a model that is one deliberate step up in complexity.

\begin{itemize}
    \item \textbf{Learning symmetric half-spaces.}
        We replace the single Abs unit with \emph{two independent ReLU neurons}, whose outputs are then summed by a fixed, non-trainable linear layer. This gives the network just enough freedom to \emph{discover} the geometric symmetry that the Abs unit had built-in, forcing it to learn how to coordinate two independent components.

    \item \textbf{A richer solution space}
        This architecture introduces a more complex learning challenge. While an ideal outcome is a solution \emph{functionally equivalent} to the Abs unit, the independent parameters allow for a \emph{family of solutions} that achieve this goal. However, this flexibility is also a vulnerability; the neurons can fail to coordinate, leading to suboptimal \textbf{local minima} far from the ideal geometry.

    \item \textbf{A miniature laboratory for learning dynamics}
        This model strikes a deliberate balance; it is complex enough to fail in non-trivial ways, exhibiting sensitivity to initialization and convergence issues, yet simple enough for its internal state to be fully analyzed. The two-dimensional input space allows every learned hyperplane to be visually inspected. This provides a tractable environment to connect abstract failure modes to concrete geometry, letting us develop intuitions that may offer insight into similar challenges in larger, more opaque networks.
    \item \textbf{Toward reliability}
        We will first establish a baseline to measure how often this more flexible model fails. We then introduce a suite of lightweight interventions-from geometry-aware initializations to runtime monitoring-to see which tactics can successfully guide the two neurons toward a coordinated solution and push the success rate toward certainty.
\end{itemize}

By the end of the chapter, we will have a clearer view of how a network \emph{just complex enough to learn XOR but no more} behaves, providing insight that will serve us well as we scale up in later work.