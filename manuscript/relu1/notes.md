Below is how I currently picture the **ReLU‑1 chapter** in light of your numbered list and the way **Abs‑1** was organized.

| #      | Working section heading          | What it would contain                                                                                                                                                                                                                                                                   | Comments on overlap / repetition                                                                                                                  |
| ------ | -------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1**  | **Using This Chapter**           | *Same role as before.* Quick‑start guide for skim‑vs‑deep readers; bullets showing where key plots/tables live and a reminder that this chapter builds on—but can be read without—the Abs‑1 chapter.                                                                                    | One‑sentence note: “Core XOR definitions from Ch. X are briefly restated below for convenience.”                                                  |
| **2**  | **Motivation**                   | Why study the *free* two‑ReLU sum after seeing the hard‑wired Abs unit. Points to cover: <br>• Extra expressiveness ⇒ new failure modes.<br>• Same end‑goal (XOR), but now learning symmetry rather than baking it in.<br>• How this foreshadows deeper nets where Abs isn’t available. | Framed as “slightly more complex, same concept,” echoing your wording.                                                                            |
| **3**  | **Model & Data**                 | Formal define: `y = Σ ReLU(wᵢ·x + bᵢ)` with two units; centred XOR inputs; label encoding. <br>Include the analytic weight solutions for XOR (optional appendix).                                                                                                                       | Begin with: *“The next two paragraphs repeat Ch. X’s dataset description so you don’t have to flip pages; feel free to skip if you remember it.”* |
| **4**  | **Experimental Framework**       | Re‑import the same protocol block (seeds, epoch limits, metrics, early‑stop). Explicitly say: “Framework identical to Abs‑1; reproduced here verbatim for reference.”                                                                                                                   | Keeps the book self‑contained.                                                                                                                    |
| **5**  | **Baseline: ReLU‑1 + Kaiming**   | Show poor mean accuracy; histogram of outcomes; geometry plots (dead data, overlapping planes). Serves as anchor for later fixes.                                                                                                                                                       | Direct analytic comparison to Abs‑1 success rate.                                                                                                 |
| **6**  | **Activation Survey**            | Sub‑sections:<br> • Leaky (α = +1, +0.1, +0.01, 0, –0.01, –0.1, –1).<br> • ELU.<br> • PReLU (learned α). <br>Focus on how non‑zero negative slope/stable tail rescues dead data.                                                                                                        | Each variant inherits framework; fold results into one composite figure/table.                                                                    |
| **7**  | **Re‑initialisation Strategies** | 7 a. *Simple dead‑data reinit* (no margin)<br>7 b. *Margin‑based reinit* (threshold 0.3). <br>Discuss detection criterion and proportion of runs needing restarts.                                                                                                                      | Explicitly notes: “Percentile‑based re-init deferred to Chapter Y.”                                                                               |
| **8**  | **Bounded‑Hypersphere Init**     | Describe tangent‑sphere idea; show that all samples start active; compare success rate and speed vs. Kaiming.                                                                                                                                                                           |                                                                                                                                                   |
| **9**  | **Runtime Monitors**             | Dead‑sample + weight‑norm bounds monitor; early abort/repair stats; compare compute saved and accuracy.                                                                                                                                                                                 |                                                                                                                                                   |
| **10** | **Loss‑Entropy Annealing**       | Detail entropy metric, noise injection, and convergence effect; highlight cases where it succeeds after Kaiming fails.                                                                                                                                                                  |                                                                                                                                                   |
| **11** | **Mirror Initialization**        | Opposite normals init; geometry illustration; success metrics; relation to Abs symmetry.                                                                                                                                                                                                |                                                                                                                                                   |
| **12** | **Take‑aways**                   | Bullet synthesis: what fixes work best, cost vs. benefit, lessons for deeper nets.                                                                                                                                                                                                      |                                                                                                                                                   |
| **13** | **Bibliography**                 | Local to chapter (`\printbibliography[heading=subbibliography]`).                                                                                                                                                                                                                       |                                                                                                                                                   |

### Key design choices I’m tracking

* **Self‑containment vs. redundancy:**
  We consciously duplicate the minimal XOR + metric definitions inside Section 3, flagged as “repeat for convenience,” so the reader is never forced to leaf back.

* **Narrative arc:**
  Baseline failure → activation tweaks → init tweaks → online fixes → hybrid tricks → lessons. Mirrors Abs‑1 flow but adds branches where new failure modes necessitate new remedies.

* **Deferred material:**
  Percentile re‑init and its supporting analysis are explicitly promised for a later chapter so the reader knows the omission is deliberate.

If this matches your intent, great—otherwise tell me where my understanding is off and I’ll adjust.
