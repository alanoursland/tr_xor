## Core Connections 

### **1. Prototype Theory & Learning**

* **Cognitive Psychology & Computational Models:**
    * Nosofsky, R. M., Sanders, C. A., Meagher, B. J., & Douglas, B. J. (2018). Toward the Development of a Feature-Space Representation for a Comprehensive Updated Version of the GCM. *Behavior research methods, 50*(1), 164-190.
    * Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2019). Human-level concept learning through probabilistic program induction. *Science, 347*(6226), 1332-1335. (Though originally 2015, its influence on concept learning and prototypes is significant and ongoing).
    * Battleday, R. M., Peterson, J. C., & Griffiths, T. L. (2020). Learning structures for generalization, abstraction and learning-to-learn. *Current Opinion in Behavioral Sciences, 32*, 60-67.

* **Computational Prototype Learning (LVQ, SOMs, Deep Learning adaptations):**
    * Jaiswal, A., Kumar, A., Kumar, S., & Chouhan, M. (2024). Prototypical Networks for Few-Shot Learning: A Survey. *ACM Computing Surveys, 56*(8), 1-38.
    * Chen, Z., Liu, Y., Stone, M., Ott, M., & Van der Maaten, L. (2021). A new generation of self-organizing maps. *Advances in Neural Information Processing Systems, 34*, 26199-26211.
    * Karaletsos, T., & Belongie, S. (2019). Deep-ROCS: Actively learning robust feature spaces with differentiable ROC-based supervision. *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 692-701.
    * Yang, H.-M., Zhang, X.-Y., Yin, F., Yang, Q., & Liu, C.-L. (2021). Convolutional Prototype Network for Open-Set Recognition. *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, *43*(12), 4435–4448.
    * Yang, H.-M., Zhang, X.-Y., Yin, F., & Liu, C.-L. (2018). Robust classification with convolutional prototype learning. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)* (pp. 3474-3482).
    * Huang, L., Huang, Y., Ouyang, W., & Wang, L. (2020). Relational Prototypical Network for Weakly Supervised Temporal Action Localization. In *Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)* (pp. 11053-11060). AAAI Press.

### **2. Geometric Deep Learning & Interpretability** 

* **Understanding Learned Representations: Similarity, Alignment, and Structure:**
    * Kornblith, S., Norouzi, M., Lee, H., & Hinton, G. (2019). Similarity of Neural Network Representations Revisited. In *Proceedings of the 36th International Conference on Machine Learning (ICML)* (Vol. 97, pp. 3519-3529). PMLR.
    * Davis Brown, Nikhil Vyas, Yamini Bansal. *On Privileged and Convergent Bases in Neural Network Representations*. Workshop on High-dimensional Learning Dynamics at ICML 2023.
    * Loek van Rossem, Andrew M. Saxe. *When Representations Align: Universality in Representation Learning Dynamics*. ICML 2024.
    * Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017). *Understanding deep learning requires re-thinking generalization*. arXiv preprint arXiv:1611.03530v2.
    * Papyan, V., Han, X. Y., & Donoho, D. L. (2020). Prevalence of Neural Collapse in Deep Learning Training. *Proceedings of the National Academy of Sciences (PNAS)*, *117*(40), 24652-24663.
    * Andriopoulos, G., Dong, Z., Guo, L., Zhao, Z., & Ross, K. (2024). *The Prevalence of Neural Collapse in Neural Multivariate Regression*. arXiv preprint arXiv:2409.04180v1.

* **Manifold Learning & Intrinsic Dimensionality:**
    * Pope, P., Hirn, M. J., & Romano, Y. (2021). The intrinsic dimension of images and its impact on learning. *Journal of Machine Learning Research, 22*(215), 1-44.
    * Ansuini, A., Laio, A., Pelliccia, J. F., & ZOCCANTE, L. (2019). Intrinsic dimension of data sets: a simple and efficient method based on statistical mechanics. *Scientific reports, 9*(1), 11033.
    * He, K., & Yu, H. (2023). Understanding the Role of Intrinsic Dimension in Deep Learning. *International Conference on Machine Learning*.

* **Decision Boundary Analysis & Network Geometry:**
    * Antognini, J., & Faltings, B. (2021). Analyzing the decision boundaries of neural networks in latent space. *Proceedings of the AAAI Conference on Artificial Intelligence, 35*(8), 6731-6739.
    * Stephenson, C., & Sud, A., & Rubenstein, P. K., & Hoffman, M. D. & Lakshminarayanan, B. (2021). On the Geometry of Neural Network Loss Surfaces: A Study of Overparameterized Models. *International Conference on Learning Representations*.
    * Vardanega, J., Traina, A. J. M., & Traina Jr, C. (2022). A Survey on Decision Boundary for Neural Networks. *IEEE Access, 10*, 123554-123575.

* **Capsule Networks (focusing on geometric understanding):**
    * Sabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic routing between capsules. *Advances in neural information processing systems, 30*. (Foundational)
    * Wang, D., & Liu, G. (2021). A survey of capsule networks. *Artificial Intelligence Review, 54*(2), 1271-1298.
    * Mazzia, V., Salvetti, F., & Chiaberge, M. (2021). Efficient-CapsNet: Capsule Network with Self-Attention Routing. *Scientific Reports, 11*(1), 1-13.

### **3. Distance and Similarity Learning** 

* **Explicit Distance Learning & Inductive Biases for Metric Properties:**
    * Silviu Pitis, Harris Chan, Kiarash Jamali, Jimmy Ba. *An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality*. ICLR 2020 Conference.
    * Kaya, M., & Bilge, H. Ş. (2019). Deep metric learning: A survey. *Symmetry, 11*(9), 1066.
    * Musgrave, K., Belongie, S., & Lim, S. N. (2020). A metric learning reality check. *European conference on computer vision*, 681-699.
    * Roth, K., Brattoli, B., & Ommer, B. (2022). Revisiting training strategies and generalization performance in deep metric learning. *International Conference on Machine Learning*, 18678-18702.
    * Deng, J., Guo, J., Yang, J., Xue, N., Kotsia, I., & Zafeiriou, S. (2022). ArcFace: Additive Angular Margin Loss for Deep Face Recognition. *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, *44*(10), 5962–5979.
    * Wang, H., Wang, Y., Zhou, Z., Ji, X., Gong, D., Zhou, J., Li, Z., & Liu, W. (2018). *CosFace: Large Margin Cosine Loss for Deep Face Recognition*. arXiv preprint arXiv:1801.09414v2.

* **Self-Supervised and Contrastive Learning for Similarity Representations:**
    * Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton. *A Simple Framework for Contrastive Learning of Visual Representations*. In International Conference on Machine Learning (pp. 1597–1607), PMLR, November 2020.
    * Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Nicolas Ballas, et al. *Self-supervised Learning from Images with a Joint-Embedding Predictive Architecture*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 15619–15629), 2023.
        * *Relevance:* This paper explores joint-embedding architectures for self-supervised learning, which learn representations by predicting one view of an image from another, thereby creating an embedding space where proximity indicates semantic similarity.
    * Hoffer, E., & Ailon, N. (2015). Deep metric learning using triplet network. *International Workshop on Similarity-Based Pattern Recognition*, 84-92. (Foundational for deep learning era)
    * Xuan, H., Liu, R., & Vassiliadis, S. (2020). A survey of triplet loss and its variants for deep metric learning. *IEEE Transactions on Pattern Analysis and Machine Intelligence, 43*(10), 3367-3385.
    * Schroff, F., Kalenichenko, D., & Philbin, J. (2015). FaceNet: A unified embedding for face recognition and clustering. *Proceedings of the IEEE conference on computer vision and pattern recognition*, 815-823. (Popularized triplet loss)

* **Relationship to Kernel Methods/SVMs in the Deep Learning Era:**
    * Belkin, M., Hsu, D., Ma, S., & Mandal, S. (2019). Reconciling modern machine-learning practice and the classical bias–variance trade-off. *Proceedings of the National Academy of Sciences, 116*(32), 15849-15854. (Discusses how overparameterized NNs can behave like kernel machines).
    * Liang, T., Poggio, T., Rakhlin, A., & Stokes, J. (2020). Fisher-Rao metric, geometry, and complexity of neural networks. *Journal of Machine Learning Research, 21*(167), 1-50.
    * Geifman, Y., & El-Yaniv, R. (2019). Deep active learning with a neural architecture search. *Proceedings of the AAAI Conference on Artificial Intelligence, 33*(01), 3588-3595. (Contrasts learning feature hierarchies vs. fixed kernels).

---

## Creative & Expansive Connections 

### **1. Generative Models & Density Estimation**

* **GANs, VAEs (Interpreting Latent Spaces & Manifolds):**
    * Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. *Advances in neural information processing systems, 27*. (Foundational)
    * Kingma, D. P., & Welling, M. (2019). An introduction to variational autoencoders. *Foundations and Trends® in Machine Learning, 12*(4), 307-392.
    * Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., & Aila, T. (2020). Analyzing and improving the image quality of StyleGAN. *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, 8110-8119. (Focus on disentanglement and manifold properties in GANs).

* **Energy-Based Models (EBMs):**
    * LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). A tutorial on energy-based learning. *Predicting structured data, 1*(0). (Foundational)
    * Du, Y., & Mordatch, I. (2019). Implicit generation and generalization in energy-based models. *arXiv preprint arXiv:1903.08689*.
    * Grathwohl, W., Wang, K. C., Jacobsen, J. H., Duvenaud, D., Norouzi, M., & Swersky, K. (2020). Your classifier is secretly an energy based model and you should treat it like one. *Advances in Neural Information Processing Systems, 33*, 20889-20899.

### **2. Causality and Invariant Feature Learning** 

* **Disentangled Representations:**
    * Bengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new perspectives. *IEEE transactions on pattern analysis and machine intelligence, 35*(8), 1798-1828. (Early influential paper).
    * Locatello, F., Bauer, S., Lucic, M., Gelly, S., Schölkopf, B., & Bachem, O. (2019). Challenging common assumptions in the unsupervised learning of disentangled representations. *international conference on machine learning*, 4114-4124.
    * Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., ... & Lerchner, A. (2017). beta-VAE: Learning basic visual concepts with a constrained variational framework. *International Conference on Learning Representations*.

* **Causal Inference in Machine Learning:**
    * Schölkopf, B. (2022). Causality for machine learning. *Probabilistic and Causal Inference: The JP Morgan AI Research Series*, 767-839.
    * Arjovsky, M., Bottou, L., Gulrajani, I., & Lopez-Paz, D. (2019). Invariant risk minimization. *arXiv preprint arXiv:1907.02893*.
    * Goyal, A., Lamb, A., Hoffman, J., Sodhani, S., Levine, S., Bengio, Y., & Schölkopf, B. (2022). Inductive Biases for Deep Learning of Higher-Level Cognition. *Philosophical Transactions of the Royal Society A, 380*(2225), 20200068.

### **3. Neuroscience & Biological Vision** 

* **Receptive Fields & Hierarchical Processing:**
    * Yamins, D. L., & DiCarlo, J. J. (2016). Using goal-driven deep learning models to understand sensory cortex. *Nature neuroscience, 19*(3), 356-365.
    * Cadena, S. A., Denfield, G. H., Walker, E. Y., Gatys, L. A., Tolias, A. S., Bethge, M., & Ecker, A. S. (2019). Deep convolutional models improve predictions of visual cortical responses to natural images. *PLoS computational biology, 15*(4), e1006897.
    * Lindsay, G. W. (2021). Convolutional neural networks as models of the visual system: Past, present, and future. *Journal of Cognitive Neuroscience, 33*(10), 2017-2031.

* **Object Recognition & Shape Representation in the Brain:**
    * Barenholtz, E. (2019). The role of shape in human object recognition. *Current Opinion in Psychology, 26*, 1-6.
    * Kubilius, J., Bracci, S., & de Beeck, H. P. O. (2016). Deep neural networks as a computational model for human shape sensitivity. *PLoS computational biology, 12*(4), e1004869.
    * Ponce, C. R., Xiao, W., Schade, P. F., Hartmann, T. S., Kreiman, G., & Livingstone, M. S. (2019). Evolving stimuli for BOLD activation in visual cortex. *Neuron, 102*(1), 254-264.

### **4. Robustness & Adversarial Attacks** 

* **Adversarial Examples & Defenses:**
    * Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and harnessing adversarial examples. *arXiv preprint arXiv:1412.6572*. (Foundational)
    * Madry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. *International Conference on Learning Representations*.
    * Croce, F., & Hein, M. (2020). Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. *International conference on machine learning*, 2206-2216.

* **Certified Robustness & Geometric Interpretations:**
    * Cohen, J., Rosenfeld, E., & Kolter, J. Z. (2019). Certified adversarial robustness via randomized smoothing. *International conference on machine learning*, 1310-1320.
    * Salman, H., Li, J., Razenshteyn, I., Zhang, P., Zhang, H., Bubeck, S., & Yang, G. (2019). Provably robust deep learning via adversarially trained smoothed classifiers. *Advances in Neural Information Processing Systems, 32*.
    * Zhang, H., Chen, H., Xiao, C., Gowal, S., Stanforth, R., Li, B., ... & Hsieh, C. J. (2020). Towards stable and efficient training of verifiably robust neural networks. *International Conference on Learning Representations*.

### **5. Continual Learning & Catastrophic Forgetting** 

* **Methods for Mitigating Forgetting:**
    * Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hadsell, R. (2017). Overcoming catastrophic forgetting in neural networks. *Proceedings of the national academy of sciences, 114*(13), 3521-3526. (Elastic Weight Consolidation)
    * Lopez-Paz, D., & Ranzato, M. (2017). Gradient episodic memory for continual learning. *Advances in neural information processing systems, 30*.
    * Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., & Wermter, S. (2019). Continual lifelong learning with neural networks: A review. *Neural Networks, 113*, 54-71.

* **Representation Learning in Continual Settings:**
    * Aljundi, R., Belagnie, M., Lin, T., Tilli, T., & Tuytelaars, T. (2019). Online continual learning with maximal interfered retrieval. *Advances in Neural Information Processing Systems, 32*.
    * Van de Ven, G. M., Siegelmann, H. T., & Tolias, A. S. (2020). Brain-inspired replay for continual learning with artificial neural networks. *Nature communications, 11*(1), 1-14.
    * Farquhar, S., & Gal, Y. (2018). Towards robust evaluations of continual learning. *arXiv preprint arXiv:1805.09733*. (Focuses on evaluation).

### **6. Explainable AI (XAI)** 

* **Feature Attribution & Concept-Based Explanations:**
    * Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. *Proceedings of the IEEE international conference on computer vision*, 618-626.
    * Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., & Sayres, R. (2018). Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). *International conference on machine learning*, 2668-2677.
    * Ghorbani, A., Wexler, J., Zou, J. Y., & Kim, B. (2019). Towards automatic concept-based explanations. *Advances in Neural Information Processing Systems, 32*.

* **Prototype-Based & Case-Based Explanations:**
    * Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., & Su, J. K. (2019). This looks like that: deep learning for interpretable image recognition. *Advances in neural information processing systems, 32*.
    * Ribeiro, M. T., Singh, S., & Guestrin, C. (2018). Anchors: High-precision model-agnostic explanations. *Proceedings of the AAAI conference on artificial intelligence, 32*(1).
    * Nauta, M., Seifert, C., & Humborg, F. (2023). From Local Explanations to Global Understanding with Explainable AI Dashboards. *ACM Computing Surveys, 56*(2), 1-39.

### **7. Abstract Algebra & Topology in Neural Networks**

* **Topological Data Analysis (TDA) for Neural Networks:**
    * Carlsson, G. (2009). Topology and data. *Bulletin of the American Mathematical Society, 46*(2), 255-308. (Foundational for TDA).
    * Rieck, B., & Leitte, H. (2019). A persistence-based framework for exploring the topology of neural network representations. *IEEE transactions on visualization and computer graphics, 26*(1), 1123-1133.
    * Naitzat, G., Zhitomirsky, L., & Lim, L. H. (2020). Topology of deep neural networks. *Journal of Machine Learning Research, 21*(184), 1-45.

* **Geometric Complexity & Linear Regions:**
    * Montúfar, G., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the Number of Linear Regions of Deep Neural Networks. In *Advances in Neural Information Processing Systems (NIPS)* (Vol. 27, pp. 2924-2932). Curran Associates, Inc.
    * Serra, T., Tjandraatmadja, C., & Ramalingam, S. (2018). Bounding and counting linear regions of deep neural networks. *International Conference on Machine Learning*, 4556-4565.
    * Hanin, B., & Rolnick, D. (2019). Complexity of linear regions in deep networks. *International Conference on Machine Learning*, 2596-2604.

